# üß† TFG - Aprendizaje Federado

üìÖ **Actualizaci√≥n:** 07/04/2024  
üîß **Rama:** `Implementar_version_federada_local`

---

## üìä Datasets

Se han incorporado los datasets **MNIST** y **CIFAR10**. Para elegir cu√°l utilizar, se ha definido una variable `dataset` que puede tomar los valores:
- `0` ‚Üí MNIST
- `1` ‚Üí CIFAR10

La funci√≥n `get_dataset(dataset: int)` se encarga de:
- Cargar el dataset correspondiente
- Adaptar las im√°genes al formato requerido por ResNet18
- Calcular y aplicar la normalizaci√≥n de forma autom√°tica

---

## üîç Implementaci√≥n de ResNet18 + ROLANN

Se utiliza una **ResNet18 preentrenada** como extractor de caracter√≠sticas.  
La capa `fc` final ha sido reemplazada por una capa identidad (`nn.Identity()`), lo que permite obtener directamente el **vector de caracter√≠sticas** de la imagen.

La clasificaci√≥n se realiza con el modelo **ROLANN**, que aprende a partir de dichos vectores extra√≠dos.

---

### üóÇÔ∏è Adaptaci√≥n y Normalizaci√≥n de Datos

#### ‚úÖ Preprocesamiento autom√°tico
- Se ha calculado la **media** y la **desviaci√≥n t√≠pica** de los valores de p√≠xel dividiendo primero las im√°genes por 255 (para llevar sus valores al rango [0, 1]). Luego se calcula el promedio y la desviaci√≥n de todos los p√≠xeles del dataset, incluyendo todas las im√°genes, la altura y la anchura **axis=(0,1,2)**:

  ```python
  mean = (dataset.data / 255).mean(axis=(0, 1, 2))
  std = (dataset.data / 255).std(axis=(0, 1, 2))
  ```

  Esto permite obtener una referencia com√∫n de brillo y contraste que se usar√° despu√©s para normalizar las im√°genes. As√≠, los datos estar√°n centrados y distribuidos de forma estable para que la red los entienda mejor.

- En el caso de **MNIST**, al tener un √∫nico canal, se replica el valor 3 veces para simular una imagen RGB compatible con ResNet18:

  ```python
  mean_rgb = [mean] * 3
  std_rgb = [std] * 3
  ```

- Estos valores se utilizan posteriormente en la transformaci√≥n de normalizaci√≥n con:

  ```python
  transforms.Normalize(mean_rgb, std_rgb)
  ```

#### üñºÔ∏è MNIST
- Im√°genes originalmente en B/N (1 canal, 28x28)
- Adaptadas para ResNet18:
  - Redimensionadas a **224x224**
    ```python
    transforms.Resize((224, 224))
    ```
  - Convertidas a **RGB** para que tengan 3 canales:
    ```python
    transforms.Grayscale(num_output_channels=3)
    ```

#### üñºÔ∏è CIFAR10
- Im√°genes ya en RGB, solo se redimensionan:
  ```python
  transforms.Resize((224, 224))
  ```

---

### ‚öôÔ∏è Congelaci√≥n y Modo Evaluaci√≥n de ResNet18

- La ResNet se congela para que no se actualicen sus pesos durante el entrenamiento:
  ```python
  for param in resnet.parameters():
      param.requires_grad = False
  ```

- Se activa el modo evaluaci√≥n para que la ResNet deje de comportarse como si estuviera entrenando. Esto hace que capas como BatchNorm no cambien
  su funcionamiento y mantengan los resultados estables:
  ```python
  resnet.eval()
  ```

  ResNet18 contiene capas llamadas **Batch Normalization**, que ajustan internamente los datos con cada pasada.  
  Aunque congelemos los pesos del modelo, estas capas **siguen actualizando estad√≠sticas internas** (como la media y desviaci√≥n) cada vez que se le pasan im√°genes, por ejemplo en:
  ```python
  for x, y in train_loader:
      caracteristicas = resnet(x)
  ```
  Esto puede provocar que las caracter√≠sticas extra√≠das cambien ligeramente con cada iteraci√≥n, afectando al modelo ROLANN.

  Usar `resnet.eval()` **frena esa actualizaci√≥n autom√°tica**, asegurando que la salida de ResNet sea siempre la misma durante el entrenamiento.

- Durante la extracci√≥n de caracter√≠sticas, se desactiva el c√°lculo de gradientes:
  ```python
  with torch.no_grad():
      # Extracci√≥n de caracter√≠sticas
  ```

---

## ‚ö° Uso de GPU

Se utiliza GPU si est√° disponible, tanto para ResNet como para ROLANN:
```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
```

---

## üöÄ **Implementaci√≥n Federada**

### üîÑ Divisi√≥n del dataset entre clientes
El dataset se divide equitativamente entre m√∫ltiples clientes:
```python
dataset_dividido = random_split(dataset, batch_size)
```

### üì° Cliente
Cada cliente posee:
- Una instancia propia de ResNet18 (congelada y en modo evaluaci√≥n).
- Una instancia propia del modelo ROLANN.

Cada cliente entrena localmente su instancia de ROLANN usando las caracter√≠sticas extra√≠das por su ResNet local y posteriormente env√≠a sus matrices locales (`M`, `U`, `S`) al Coordinador.

### üóÉÔ∏è Coordinador
- Recibe y acumula matrices (`M`, `U`, `S`) de cada cliente.
- Realiza la agregaci√≥n global usando SVD sobre las matrices recibidas.
- Actualiza la instancia global del modelo ROLANN con las matrices agregadas.

### üõ†Ô∏è Manejo de dispositivos
- El coordinador asegura que todas las matrices se encuentren en el mismo dispositivo (CPU/GPU) antes de realizar c√°lculos, evitando errores de incompatibilidad.
```python
mg_tensor_list = [m if isinstance(m, torch.Tensor) else torch.tensor(m, device=self.device) for m in mg_list]
```

---

## üìà Resultados

### **22/03** (antes de normalizar correctamente):

- **MNIST**:

| M√©trica            | Valor   |
|--------------------|---------|
| Training Accuracy  | 0.9475  |
| Test Accuracy      | 0.9492  |

- **CIFAR10**:

| M√©trica            | Valor   |
|--------------------|---------|
| Training Accuracy  | 0.7740  |
| Test Accuracy      | 0.7703  |

---

### **25/03** (con normalizaci√≥n autom√°tica y mejoras):

- **MNIST**:

| M√©trica            | Valor   |
|--------------------|---------|
| Training Accuracy  | 0.9678  |
| Test Accuracy      | 0.9664  |

- **CIFAR10**:

| M√©trica            | Valor   |
|--------------------|---------|
| Training Accuracy  | 0.8460  |
| Test Accuracy      | 0.8372  |

---

### Versi√≥n Federada (07/04):

- **MNIST**:

| M√©trica            | Valor   |
|--------------------|---------|
| Training Accuracy  | 0.9685  |
| Test Accuracy      | 0.9673  |

- **CIFAR10**:

| M√©trica            | Valor   |
|--------------------|---------|
| Training Accuracy  | 0.8459  |
| Test Accuracy      | 0.8384  |


### Resultados de la Experimentaci√≥n

#### Dataset: MNIST

|   N√∫mero de Clientes | Tipo de Partici√≥n   |   Training Accuracy |   Test Accuracy |
|----------------------|---------------------|---------------------|-----------------|
|                    4 | Dirichlet (Œ±=0.3)   |              0.9684 |          0.9664 |
|                    4 | Class Less          |              0.6957 |          0.6948 |
|                    8 | Dirichlet (Œ±=0.3)   |              0.9687 |          0.967  |
|                    8 | Class Less          |              0.9524 |          0.9485 |
|                   12 | Dirichlet (Œ±=0.3)   |              0.9686 |          0.967  |
|                   12 | Class Less          |              0.9585 |          0.9577 |

#### Dataset: CIFAR-10

|   N√∫mero de Clientes | Tipo de Partici√≥n   |   Training Accuracy |   Test Accuracy |
|----------------------|---------------------|---------------------|-----------------|
|                    4 | Dirichlet (Œ±=0.3)   |              0.8456 |          0.8376 |
|                    4 | Class Less          |              0.6783 |          0.6748 |
|                    8 | Dirichlet (Œ±=0.3)   |              0.8455 |          0.8379 |
|                    8 | Class Less          |              0.8238 |          0.8167 |
|                   12 | Dirichlet (Œ±=0.3)   |              0.8457 |          0.838  |
|                   12 | Class Less          |              0.8227 |          0.8108 |


---

## ‚úÖ Conclusiones

- La incorporaci√≥n de la **normalizaci√≥n autom√°tica** basada en el propio dataset mejora significativamente el rendimiento, especialmente en el caso de CIFAR10.
- La implementaci√≥n federada muestra un rendimiento equivalente o ligeramente mejor comparado con el enfoque centralizado, validando as√≠ su eficacia.

## üìñ Bibliograf√≠a y Fuentes

- [Eliminar capa de clasificacion en RESNET](https://stackoverflow.com/questions/52548174/how-to-remove-the-last-fc-layer-from-a-resnet-model-in-pytorch)
- [Transformaciones en datasets](https://pytorch.org/vision/0.9/transforms.html)
- [Uso de random_split](https://pytorch.org/docs/stable/data.html)