# üß† TFG - Aprendizaje Federado

üìÖ **Actualizaci√≥n:** 25/03  
üîß **Rama:** `feature/Implementar_vector_caracteristicas`

---

## üìä Datasets

Se han incorporado los datasets **MNIST** y **CIFAR10**. Para elegir cu√°l utilizar, se ha definido una variable `dataset` que puede tomar los valores:
- `0` ‚Üí MNIST
- `1` ‚Üí CIFAR10

La funci√≥n `get_dataset(dataset: int)` se encarga de:
- Cargar el dataset correspondiente
- Adaptar las im√°genes al formato requerido por ResNet18
- Calcular y aplicar la normalizaci√≥n de forma autom√°tica

---

## üîç Implementaci√≥n de ResNet18 + ROLANN

Se utiliza una **ResNet18 preentrenada** como extractor de caracter√≠sticas.  
La capa `fc` final ha sido reemplazada por una capa identidad (`nn.Identity()`), lo que permite obtener directamente el **vector de caracter√≠sticas** de la imagen.

La clasificaci√≥n se realiza con el modelo **ROLANN**, que aprende a partir de dichos vectores extra√≠dos.

---

### üóÇÔ∏è Adaptaci√≥n y Normalizaci√≥n de Datos

#### ‚úÖ Preprocesamiento autom√°tico
- Se ha calculado la **media** y la **desviaci√≥n t√≠pica** de los valores de p√≠xel dividiendo primero las im√°genes por 255 (para llevar sus valores al rango [0, 1]). Luego se calcula el promedio y la desviaci√≥n de todos los p√≠xeles del dataset, incluyendo todas las im√°genes, la altura y la anchura **axis=(0,1,2)**:

  ```python
  mean = (dataset.data / 255).mean(axis=(0, 1, 2))
  std = (dataset.data / 255).std(axis=(0, 1, 2))
  ```

  Esto permite obtener una referencia com√∫n de brillo y contraste que se usar√° despu√©s para normalizar las im√°genes. As√≠, los datos estar√°n centrados y distribuidos de forma estable para que la red los entienda mejor.

- En el caso de **MNIST**, al tener un √∫nico canal, se replica el valor 3 veces para simular una imagen RGB compatible con ResNet18:

  ```python
  mean_rgb = [mean] * 3
  std_rgb = [std] * 3
  ```

- Estos valores se utilizan posteriormente en la transformaci√≥n de normalizaci√≥n con:

  ```python
  transforms.Normalize(mean_rgb, std_rgb)
  ```

#### üñºÔ∏è MNIST
- Im√°genes originalmente en B/N (1 canal, 28x28)
- Adaptadas para ResNet18:
  - Redimensionadas a **224x224**
    ```python
    transforms.Resize((224, 224))
    ```
  - Convertidas a **RGB** para que tengan 3 canales:
    ```python
    transforms.Grayscale(num_output_channels=3)
    ```

#### üñºÔ∏è CIFAR10
- Im√°genes ya en RGB, solo se redimensionan:
  ```python
  transforms.Resize((224, 224))
  ```

---

### ‚öôÔ∏è Congelaci√≥n y Modo Evaluaci√≥n de ResNet18

- La ResNet se congela para que no se actualicen sus pesos durante el entrenamiento:
  ```python
  for param in resnet.parameters():
      param.requires_grad = False
  ```

- Se activa el modo evaluaci√≥n para que la ResNet deje de comportarse como si estuviera entrenando. Esto hace que capas como BatchNorm no cambien
  su funcionamiento y mantengan los resultados estables:
  ```python
  resnet.eval()
  ```

  ResNet18 contiene capas llamadas **Batch Normalization**, que ajustan internamente los datos con cada pasada.  
  Aunque congelemos los pesos del modelo, estas capas **siguen actualizando estad√≠sticas internas** (como la media y desviaci√≥n) cada vez que se le pasan im√°genes, por ejemplo en:
  ```python
  for x, y in train_loader:
      caracteristicas = resnet(x)
  ```
  Esto puede provocar que las caracter√≠sticas extra√≠das cambien ligeramente con cada iteraci√≥n, afectando al modelo ROLANN.

  Usar `resnet.eval()` **frena esa actualizaci√≥n autom√°tica**, asegurando que la salida de ResNet sea siempre la misma durante el entrenamiento.

- Durante la extracci√≥n de caracter√≠sticas, se desactiva el c√°lculo de gradientes:
  ```python
  with torch.no_grad():
      # Extracci√≥n de caracter√≠sticas
  ```

---

## ‚ö° Uso de GPU

Se utiliza GPU si est√° disponible, tanto para ResNet como para ROLANN:
```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
```

---

## üìà Resultados

### **22/03** (antes de normalizar correctamente):

- **MNIST**:

| M√©trica            | Valor   |
|--------------------|---------|
| Training Accuracy  | 0.9475  |
| Test Accuracy      | 0.9492  |

- **CIFAR10**:

| M√©trica            | Valor   |
|--------------------|---------|
| Training Accuracy  | 0.7740  |
| Test Accuracy      | 0.7703  |

---

### **25/03** (con normalizaci√≥n autom√°tica y mejoras):

- **MNIST**:

| M√©trica            | Valor   |
|--------------------|---------|
| Training Accuracy  | 0.9678  |
| Test Accuracy      | 0.9664  |

- **CIFAR10**:

| M√©trica            | Valor   |
|--------------------|---------|
| Training Accuracy  | 0.8460  |
| Test Accuracy      | 0.8372  |

---

## ‚úÖ Conclusiones

- La incorporaci√≥n de la **normalizaci√≥n autom√°tica** basada en el propio dataset mejora significativamente el rendimiento, especialmente en el caso de CIFAR10.

## üìñ Bibliograf√≠a y Fuentes

- [Eliminar capa de clasificacion en RESNET](https://stackoverflow.com/questions/52548174/how-to-remove-the-last-fc-layer-from-a-resnet-model-in-pytorch)
- [Transformaciones en datasets](https://pytorch.org/vision/0.9/transforms.html)