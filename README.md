# üß† TFG - Aprendizaje Federado

üìÖ **Actualizaci√≥n:** 24/03  
üîß **Rama:** `feature/Implementar_vector_caracteristicas`

---

## üìä Datasets

Se han incorporado los datasets **MNIST** y **CIFAR10**. Para elegir cu√°l utilizar, se ha definido una variable `dataset` que puede tomar los valores:
- `0` ‚Üí MNIST
- `1` ‚Üí CIFAR10

La funci√≥n `get_dataset(dataset: int)` se encarga de:
- Cargar el dataset correspondiente
- Adaptar las im√°genes al formato requerido por ResNet18
- Calcular y aplicar la normalizaci√≥n de forma autom√°tica

---

## üîç Implementaci√≥n de ResNet18 + ROLANN

Se utiliza una **ResNet18 preentrenada** como extractor de caracter√≠sticas.  
La capa `fc` final ha sido reemplazada por una capa identidad (`nn.Identity()`), lo que permite obtener directamente el **vector de caracter√≠sticas** de la imagen.

La clasificaci√≥n se realiza con el modelo **ROLANN**, que aprende a partir de dichos vectores extra√≠dos.

---

### üóÇÔ∏è Adaptaci√≥n y Normalizaci√≥n de Datos

#### ‚úÖ Preprocesamiento autom√°tico
- Se ha creado la funci√≥n `get_mean_std()` que calcula de forma precisa la **media y desviaci√≥n t√≠pica por canal** sobre un dataset temporal.
- Estos valores se utilizan para normalizar cada imagen con:
  ```python
  transforms.Normalize(mean.tolist(), std.tolist())
  ```

#### üñºÔ∏è MNIST
- Im√°genes originalmente en B/N (1 canal, 28x28)
- Adaptadas para ResNet18:
  - Redimensionadas a **224x224**
    ```python
    transforms.Resize((224, 224))
    ```
  - Convertidas a **RGB** para que tengan 3 canales:
    ```python
    transforms.Grayscale(num_output_channels=3)
    ```

#### üñºÔ∏è CIFAR10
- Im√°genes ya en RGB, solo se redimensionan:
  ```python
  transforms.Resize((224, 224))
  ```

---

### ‚öôÔ∏è Congelaci√≥n y Modo Evaluaci√≥n de ResNet18

- La ResNet se congela para que no se actualicen sus pesos durante el entrenamiento:
  ```python
  for param in resnet.parameters():
      param.requires_grad = False
  ```

- Se activa el modo evaluaci√≥n para que la ResNet deje de comportarse como si estuviera entrenando. Esto hace que capas como BatchNorm no cambien
  su funcionamiento y mantengan los resultados estables:
  ```python
  resnet.eval()
  ```

  ResNet18 contiene capas llamadas **Batch Normalization**, que ajustan internamente los datos con cada pasada.  
  Aunque congelemos los pesos del modelo, estas capas **siguen actualizando estad√≠sticas internas** (como la media y desviaci√≥n) cada vez que se le pasan im√°genes, por ejemplo en:
  ```python
  for x, y in train_loader:
      caracteristicas = resnet(x)
  ```
  Esto puede provocar que las caracter√≠sticas extra√≠das cambien ligeramente con cada iteraci√≥n, afectando al modelo ROLANN.

  Usar `resnet.eval()` **frena esa actualizaci√≥n autom√°tica**, asegurando que la salida de ResNet sea siempre la misma durante el entrenamiento.

- Durante la extracci√≥n de caracter√≠sticas, se desactiva el c√°lculo de gradientes:
  ```python
  with torch.no_grad():
      # Extracci√≥n de caracter√≠sticas
  ```

---

## ‚ö° Uso de GPU

Se utiliza GPU si est√° disponible, tanto para ResNet como para ROLANN:
```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
```

---

## üìà Resultados

### **22/03** (antes de normalizar correctamente):

- **MNIST**:

| M√©trica            | Valor   |
|--------------------|---------|
| Training Accuracy  | 0.9475  |
| Test Accuracy      | 0.9492  |

- **CIFAR10**:

| M√©trica            | Valor   |
|--------------------|---------|
| Training Accuracy  | 0.7740  |
| Test Accuracy      | 0.7703  |

---

### **24/03** (con normalizaci√≥n autom√°tica y mejoras):

- **MNIST**:

| M√©trica            | Valor   |
|--------------------|---------|
| Training Accuracy  | 0.9677  |
| Test Accuracy      | 0.9658  |

- **CIFAR10**:

| M√©trica            | Valor   |
|--------------------|---------|
| Training Accuracy  | 0.8457  |
| Test Accuracy      | 0.8372  |

---

## ‚úÖ Conclusiones

- La incorporaci√≥n de la **normalizaci√≥n autom√°tica** basada en el propio dataset mejora significativamente el rendimiento, especialmente en el caso de CIFAR10.

## üìñ Bibliograf√≠a y Fuentes

- [Calculo media y desviacion tipica](https://www.youtube.com/watch?v=y6IEcEBRZks)
- [Eliminar capa de clasificacion en RESNET](https://stackoverflow.com/questions/52548174/how-to-remove-the-last-fc-layer-from-a-resnet-model-in-pytorch)
- [Transformaciones en datasets](https://pytorch.org/vision/0.9/transforms.html)